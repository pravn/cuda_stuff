Experiments in CUDA.
Sources: Wen-Mei Hwu's book, Shane Cook's book and parallelforall blog.

1) matmul_naive: Trivial matrix multiplication with no attempt at parallelization.
2) matmul_shem: Tiling and shared memory (incomplete).
3) ballot - attempt at using ballot+popc to replace atomics. Single block.
	- ballot_atomicadd1 uses an atomicOr to collect ballot return.
	- ballot_atomicadd2 stores results of ballot in register and reduces 
4) shffl - shffl instructions to do reductions. 

5) histogram - using ballot and atomics to compute histogram. ballot
	implementation simply adds an extra array dimension to 3) for each
	bin. Check later to compare speed with that of atomicAdd.

6) thrust - Pulled out stream compaction sample from thrust repo. Ignorable for now.
   	    Later investigations should tell us how atomics work with thrust.

7) vectorized_loads - This is a copy of Justin Luitjens' suggestion in the CUDA parallelforall blog
   		    Reading global memory data in vectorized fashion seems to work nicely. 
		    But I am not quite sure if the bandwidth numbers produced by code are 'correct'. 



=============================================================================
Results and temorary notes pased from last commit
Histogram computations compared for popc+ballot vs atomicAdd in shared memory 
=============================================================================

We have N threads writing to a small number of bins M.
The ballot+popc histogram is implemented by doing warp level reductions using
ballot and popc.

It performs much faster than the naked atomicAdd for the case where
all threads write to a single bin. However, its performance degrades
with number of bins. This could possibly because of it being a one dimensional
kernel, whereupon it might happen that the single thread incrementing M bin counters
might be overwhelemed when it has to handle a large number of bin variables.

The next step is therefore to parallelize the bin counter loop in the Y direction
to use a thread for each bin counter.

For now, it appears that the popc histogram is successful in that it overcomes
some of the contention limitations of the atomicAdd histogram.

TODO
	Multiblock kernels for ballot, shuffl, histogram and atomics.
	Atomics in shared memory and private per thread histograms from
	Nicholas Wilt's article online.
	Timing experiments for various tests. In particular

	Atomics performance where threads write to a small number of bins.
	The hope is make an evaluation of various methods and pronounce which
	one of them is the most performant.

	a) Naive and Shared memory atomics
	b) Per thread histogram as per Wilt
	c) Ballot+Popc followed by a subsequent reduction.
	
