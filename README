Experiments in CUDA.
Sources: Wen-Mei Hwu's book, Shane Cook's book and parallelforall blog.

1) matmul_naive: Trivial matrix multiplication with no attempt at parallelization.
2) matmul_shem: Tiling and shared memory (incomplete).
3) ballot - attempt at using ballot+popc to replace atomics. Single block.
	- ballot_atomicadd1 uses an atomicOr to collect ballot return.
	- ballot_atomicadd2 stores results of ballot in register and reduces 
4) shffl - shffl instructions to do reductions. 

5) histogram - using ballot and atomics to compute histogram. ballot
	implementation simply adds an extra array dimension to 3) for each
	bin. Check later to compare speed with that of atomicAdd.

6) thrust - Pulled out stream compaction sample from thrust repo. Ignorable for now.
   	    Later investigations should tell us how atomics work with thrust.

7) vectorized_loads - This is a copy of Justin Luitjens' suggestion in the CUDA parallelforall blog
   		    Reading global memory data in vectorized fashion seems to work nicely. 
		    But I am not quite sure if the bandwidth numbers produced by code are 'correct'. 



=============================================================================
Results and temorary notes pased from last commit
Histogram computations compared for popc+ballot vs atomicAdd in shared memory 
=============================================================================

We have N threads writing to a small number of bins M.
The ballot+popc histogram is implemented by doing warp level reductions using
ballot and popc.

It performs much faster than the naked atomicAdd for the case where
all threads write to a single bin. However, its performance degrades
with number of bins. This could possibly because of it being a one dimensional
kernel, whereupon it might happen that the single thread incrementing M bin counters
might be overwhelemed when it has to handle a large number of bin variables.

The next step is therefore to parallelize the bin counter loop in the Y direction
to use a thread for each bin counter.

For now, it appears that the popc histogram is successful in that it overcomes
some of the contention limitations of the atomicAdd histogram.

TODO
	Multiblock kernels for ballot, shuffl, histogram and atomics.
	Atomics in shared memory and private per thread histograms from
	Nicholas Wilt's article online.
	Timing experiments for various tests. In particular

	Atomics performance where threads write to a small number of bins.
	The hope is make an evaluation of various methods and pronounce which
	one of them is the most performant.

	a) Naive and Shared memory atomics
	b) Per thread histogram as per Wilt
	c) Ballot+Popc followed by a subsequent reduction.
	
UPDATE: Sunday, Oct 16 2016
---------------------------
A warp level atomicAdd is now included. This gives 2X improvements in Kepler (sm_35, K6000),
but in Maxwell, we don't find any performance difference between the vanilla shared memory
atomicAdd and the warp level atomicAdd. It might therefore appear that Maxwell's atomicAdd 
somehow has this factored in. This is for the case where the input is extremely degenerate,
with all data being written to bin 0.

There are a few more ideas to try to improve atomics performance for highly degenerate 
input.

1) Block level sort + count - the sorting algorithm is to be understood and chosen. Some
	code exits in the NVIDIA samples for bitonic sort. 

2) Privatized histograms - It was conjectured that there won't be enough shared memory 
   for thread level histograms in shared memory. However, it would be interesting 
   to investigate privatized histograms in global memory. 

3) Ballot popc warp level histograms - It was concluded earlier that ballot popc 
   needs more parallelism to compute several bins. The parallelism can be 
   created by, for example, replicating the problem for each bin. In earlier 
   experiments, I had concluded that replicating the problem causes contention 
   in reading input data. However, I now see that we can also replicate 
   the input data to do away with contention. This should be investigated.





	
	


